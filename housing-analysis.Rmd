---
title: "King County Housing Price Analysis"
author: "Hoa Nguyen, Minh Le, Yen Nguyen"
date: "2023-09-22"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(reshape2)   
library(randomForest)
kc_house_data <- read_csv("kc_house_data.csv")
```
## Introduction

King County is the most populous county in Washington. It is a vibrant metropolis known for its innovation and rich culture. King County is a key component of the Seattle-Tacoma-Bellevue metropolitan area, a hub of both economic activity and cultural diversity.

In this project, we would use the data set that shape the landscape of "house sale prices" in King County. By delving into this dataset, we aim to understand how various factors impose impacts on property values in King County. Through statistical analyses and data visualization, we seek to gain insights that inform both the past and future of real estate in this county.

The data set includes homes sold between May 2014 and May 2015 in King County. The original dataset has 21 features of houses; however, we only use the following features about housing in this project:

- price: Price of each home sold

- bedrooms: Number of bedrooms

- bathrooms: Number of bathrooms, where .5 accounts for a room with a toilet but no shower

- sqft_living: Square footage of the apartments interior living space

- sqft_lot: Square footage of the land space

- floors: Number of floors

- waterfront: A dummy variable for whether the apartment was overlooking the waterfront or not

- view: An index from 0 to 4 of how good the view of the property was

- condition: An index from 1 to 5 on the condition of the apartment,

- grade: An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.

- sqft_above: The square footage of the interior housing space that is above ground level

- sqft_basement: The square footage of the interior housing space that is below ground level

- yr_built: The year the house was initially built

- yr_renovated: The year of the house’s last renovation

- zipcode: What zipcode area the house is in

Harlfoxem. “House Sales in King County, USA.” Kaggle, August 25, 2016. https://www.kaggle.com/datasets/harlfoxem/housesalesprediction/data.  


Ethical Considerations
When working with this dataset, it is essential to prioritize privacy and confidentiality. Since it contains various details about homes, including their exact locations, prices, dimensions, conditions, and more, there is a need to anonymize any data that could reveal individual identities to protect privacy rights. Furthermore, it is crucial to ensure that data collection adheres to informed consent principles and legal regulations, respecting individuals' rights. For the analysis to be fair and objective, any biases in pricing or practices that can unfairly disadvantage some groups should be addressed and mitigated. Another important factor to consider is transparency, which can be achieved by thoroughly documenting data sources and methodologies to enhance the credibility of the analysis and uphold ethical standards throughout the research process. By embracing these ethical principles, researchers can conduct their study in a responsible and respectful manner that contributes to the accuracy and significance of their predictive analysis of house prices.

Data Exploration

First, we can look into the summary statistics of the variables that we used in the dataset: 

```{r, echo = FALSE}
kc_house_filtered = kc_house_data %>% select(id, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, zipcode)
summary(kc_house_filtered)

```

```{r, echo = FALSE}
kc_house_filtered %>%
  group_by(zipcode) %>%
  summarize(mean_price = mean(price, na.rm = TRUE)) %>%
  arrange(desc(mean_price))

```
Area with the zip code 98039 has the highest average housing price in King County of 2160606 dollars, followed by areas 98004, 98040, and 98112 with the price range from 1000000 dollars to 1350000 dollars. This implies that selling houses in these areas will be at a higher price than selling in other places in King County.





```{r}
by(kc_house_data$price, kc_house_data$waterfront, summary)
```

Based on the above results, the average price for houses overlooking the waterfront is nearly triple the price of non-waterfront houses. This suggests that the presence of a waterfront view somewhat has an impact on the prices. However, the maximum price of houses without waterfront is higher than that of houses overlooking waterfront, at 7700000 and 7062500, respectively. This means that there are some properties without waterfront views have sold for a higher maximum price compared to those with the waterfront views. This observation can be attributed to outliers within the non-waterfront group for various reasons such as recent renovations, or unique features, have sold at exceptionally high prices.

```{r}
summary(kc_house_data$price)
```

Based on the summary statistics above for the `price`, the minimum value of `price` is 75000 while the highest value is 7700000. At the 25th percentile of 321950, a significant portion of houses in King County falls below this price point. The median price at 450000 serves as a central reference point, suggesting that half of the houses are priced below this value and half above. The mean price at 540088 represents the average home price in the county. As we move to the 75th percentile of 645000, we find that a substantial portion of homes are valued above this point, emphasizing the presence of upscale housing options. 


To explore the house prices’ relationships with other variables, we generate box plots illustrating house prices in relation to their condition and view rates.

```{r}
ggplot(kc_house_data, aes(x = condition, y = log(price), fill = view)) +
  geom_boxplot(aes(group = interaction(condition, view))) +
  labs(x = "The condition of the apartment",
       y = "Log-transformed price of each home sold",
       title = "Boxplots of log-transformed price by condition and view")

```

It can clearly be seen that the majority of apartments have condition ratings of at least 3. Moreover, for houses with condition ratings above 3, there is a noticeable trend: as the view rate increases, the house prices tend to rise, which is evident from the increasing price range in the boxplots. This suggests a positive correlation between house prices and their view. Meanwhile, there is no clear association between house prices and their condition as the houses whose conditions are rated 3, 4, and 5 exhibit minimal variation in pricing. 



Statistical Analysis and Interpretation

Relationship: price ~ sqft living, sqft_lot, 
```{r}
df_filtered <- kc_house_data %>% select(price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, view, condition, grade, sqft_above, sqft_basement)
correlation_matrix <- cor(df_filtered)

ggplot(data = melt(correlation_matrix), aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(x = "", y = "", title = "Correlation Heatmap Between Variables") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
ggplot(kc_house_data, aes(x = log(price), y = sqft_living)) + 
  geom_point(color = "darkblue") +
  geom_smooth(method = "lm", color = "red") +
  labs(x = "Log Price ($)", 
       y = "Interior Living Space (sq.ft.)", 
       title = "Relationship Between Interior Living Space and Price")
```



Interpretation: From the scatter plot above, we can see that there is a positive correlation between the price of the house and the interior living space. 



Conclusions


PART 2

## Bootstrapped hypothesis test for the variances of prices of houses with low-quality and high-quality construction and design

We want to see whether the variance of house prices differs between houses with low-quality construction and design (grade < 7)  and houses with good construction and design (grade >= 7). We will conduct the following hypothesis test:

Null Hypothesis ($H_0$): The population variances of house prices are equal for the two groups (grade < 7 and grade >= 7).

Alternative Hypothesis ($H_a$): The population variances of house prices are not equal for the two groups (a two-tailed test).

Let’s choose the significance level α = 0.05.

```{r}
set.seed(123)
R <- 10000
low_quality_house <- kc_house_data %>% filter(grade < 7)
high_quality_house <- kc_house_data %>% filter(grade >= 7)

n <- nrow(low_quality_house)
m <- nrow(high_quality_house)
obs_diff <- var(high_quality_house$price) - var(low_quality_house$price)
count <- 0
for (r in 1:R) {
  low_quality_bootstrap_sample <- sample(x = low_quality_house$price, size = n, replace = TRUE)
  high_quality_bootstrap_sample <- sample(x = high_quality_house$price, size = m, replace = TRUE)
  diff <- var(high_quality_bootstrap_sample) - var(low_quality_bootstrap_sample)
  if (diff >= obs_diff) {
    count <- count + 1
  }
}
p_val <- count / R 
p_val
```

Since our p-value of 0.4879 was greater than α = 0.05, we fail to reject the null hypothesis. This means that based on the data we have and the results of our bootstrap hypothesis test, there is no significant difference in the population variances of house prices between the two groups with different grades of construction and design. Our analysis suggests that the variances in house prices for properties with a grade of less than 7 and properties with a grade of 7 or higher are similar within the sample we've analyzed.


## Bootstrapped confidence interval for the variance of the population

```{r}
set.seed(123)
R <- 1000 #number of times that we will resample
n <- nrow(kc_house_data)
vars <- c()

for(r in 1:R) {
  bootstrap_sample <- sample(x = kc_house_data$price, size = n, replace = TRUE) 
  #repeatedly draw samples of size n with replacement from our sample
  vars[r] <- var(bootstrap_sample) #calculate the mean of each of the new samples we draw
}

lower.ci <- quantile(x = vars, p = 0.025) 
#the 2.5th percentile of the 1000 bootstrapped sample averages
upper.ci <- quantile(x = vars, p = 0.975) 
#the 97.5th percentile of the 1000 bootstrapped sample averages

lower.ci
upper.ci
```
The 95% confidence interval for the variance of house prices, based on the bootstrapped samples, ranges from approximately \$124.98 billion to \$145.58 billion.

This confidence interval gives us a range of plausible values for the population variance of house prices. We are 95% confident that the true population variance is likely to fall within this interval. In other words, if I repeatedly take samples of the same size from the same population and compute a 95% CI for the variance using the same resampling method, approximately 95% of those intervals would contain the true population variance.



TEST STATISTIC FOR DIFFERENCE IN MEANS
In this test statistic, we assess the difference in average housing prices between properties with waterfront views and those without. By constructing this test statistic, we aim to evaluate whether there is a statistically significant difference in average housing prices between the two groups. 

Let $\mu_w$ denote the average price of a house with a waterfront view, and $\mu_o$ denote
the average price of houses that do not have a waterfront view. Our hypotheses are:
                              $H_0 : \mu_w = \mu_o$
                              $H_A : \mu_w \neq \mu_o$

First, let's look at the distributions of data in both data sets `with_waterfront` and `without_waterfront`. The distributions both look reasonably close to following a normal distribution.
Also, sample sizes are both large (163 for houses with waterfront and 21450 for ones without waterfront), so it is reasonable to apply the central limit theorem.

```{r}
with_waterfront <- subset(kc_house_data, waterfront == 1)
without_waterfront <- subset(kc_house_data, waterfront == 0)
hist(log(with_waterfront$price))
hist(log(without_waterfront$price))
```


Next, we perform a t-test to test whether or not the average prices of houses with waterfront view and those without waterfront view are equal. As shown in the plots above, the variances of these two groups are not identical (where the range of values for houses without waterfront are more spread out than ones with a waterfront view), so we conduct a t-test under the assumption that the variances are unequal.

```{r}
t.test(with_waterfront$price, without_waterfront$price)
```
In our comparison of prices between properties with a waterfront and without a waterfront view, we found our test statistic is $t^* = 12.876$ which is considered to be statistically significant and the difference here is unlikely to have occurred by chance.

Our p-value is less than 2.2e-16, so we have sufficiently strong evidence to reject the null hypothesis at the α = 0.05 significance level. Namely, we have sufficiently strong evidence to conclude that there is a difference between the average price of properties with a waterfront view and those that do not.

We are 95% confident that the average price for houses with a waterfront view is between 956963.3 less and 1303661.6 more than the average price for houses that do not have a waterfront view. By 95% confidence level, we mean that if we repeated the study many times, and then constructed many 95% confidence intervals for the difference in average prices between houses with and without waterfront view, then around 95% of those confidence intervals would contain the true difference.



 PART 3



## Technical conditions for SLR
- Linear function: Based on the plot between `price` and `sqft_above`, we can see that our data can roughly fit a line
- Independence of errors: The errors are independent from each other and do not follow any particular trend
- Normally distributed: As we plot the histogram, the errors are normally distributed
- Equal variances: Our errors have equal variances at each value of the predictor.

```{r}
plot(sqrt(kc_house_data$price) ~ kc_house_data$sqft_above)
price <- lm(sqrt(kc_house_data$price) ~ kc_house_data$sqft_above)
abline(price, col = "red")
plot(price$residuals~kc_house_data$sqft_above)
hist(price$residuals)

price <- lm(kc_house_data$price ~ kc_house_data$sqft_above)
summary(price)
```







## Coefficients Interpretation
According to the result above, the coefficient of `sqft_above` is 268.5, which is pretty large and positive. This means that as the value of `sqft_above` increases, the mean of `price` also tends to increase significantly. If `sqft_above` increases by one square foot, the `price` increases by \$268.5 according to the model. Also, the p-value in this case is less than 2e-16, which indicates the variable `sqft_above` has a significant influence on the `price`. 
From the results, we can have a regression equation: $price = 59953.2 + 268.5*sqft\_above \pm 2.4$




### Logistic Regression
In terms of logistic regression, we decided to use `sqft_living` to predict whether the property has a `high_price` or not. Also, we used `mutate` to create a new column to determine if the price of a particular property is above 540088 which is an average price in our data set, then it is considered to have a `high_price`; otherwise, it will be assigned 0. 

``` {r}
kc_house_data$high_price<-factor(ifelse(kc_house_data$price >= 540088,1,0))
kc_house_data <- kc_house_data %>%
  mutate(high_price = as.numeric(high_price == 1))
plot(data = kc_house_data, as.numeric(high_price) ~ sqft_living)
mod <- glm(data = kc_house_data, as.numeric(high_price) ~ sqft_living, family = binomial)
preds <- predict(mod, type = "response")

pred.df <- data.frame(sort(preds), sort(kc_house_data$sqft_living))
colnames(pred.df) <- c("probs", "sqft_living")
lines(data = pred.df, probs ~ sqft_living, col = "red")
summary(mod)
```






Based on the results of the logistic regression model, we can see that the coefficient is 1.859e-03, which is positive. As `sqft_living` increases by one square foot, the log-odds of `high_price` increases by 1.859e-03. Also, both the intercept and sqft_living coefficients are statistically significant (p < 0.001), suggesting that they make a remarkable contribution to predicting whether a property has a high price or not.
In our case, the logistic regression equation is: $\hat{p}(high\_price = 1 | sqft\_living =x) = \frac{e^{-4.513 + 1.859e-03 \times sqft\_living}}{1+e^{-4.513 + 1.859e-03 \times sqft\_living}}$



### New Technique - Random Forest 

Random Forest is a robust machine learning algorithm used for both classification and regression tasks. It constructs multiple decision trees during training, where each tree learns from random subsets of the data, and combines their outputs to make predictions. This ensemble technique helps to reduce overfitting and improve accuracy.

Here, we would like to conduct a Random Forest classification on our dataset:

1. **Data Preprocessing:** Create a new categorical variable (`price_rate`) based on the `price` column's distribution. Specifically, we categorize it into either "low" (<\$300,000), "medium" (<\$300,000 and $\leq$\$650,000),  or "high" (> \$650,000).

2. **Data Splitting:** Split the dataset into training and testing subsets (80% for training, 20% for testing) using the `sample` function.

3. **Random Forest Model Building:** Use the `randomForest` function from the `randomForest` library in R to build the model. Specify the formula (`price_rate ~ . - price - id -date`) to predict `price_rate` based on other features in the dataset.

4. **Model Evaluation:** Validate the model using the test dataset by making predictions (`predict`) and comparing them with the actual `price_rate` values. Generate a table (`table`) to analyze predicted vs. actual values.

5. **Accuracy Calculation:** Calculate the accuracy of the model's predictions by comparing predicted values with the actual `price_rate` values from the test set.

```{r}
# Create a new variable to assess the price rate 
kc_house_data$price_rate <- ifelse(kc_house_data$price <= 300000, "low", 
                                ifelse(kc_house_data$price > 300000 & kc_house_data$price <= 650000, "medium", "high"))

kc_house_data$price_rate <- as.factor(kc_house_data$price_rate)

# Split the data into training and testing. 80% for training, 20% for testing.
set.seed(123)
samp <- sample(nrow(kc_house_data), 0.8 * nrow(kc_house_data))
train <- kc_house_data[samp, ]
test <- kc_house_data[-samp, ]

# Checks the dimensions of training and testing dataset
dim(train)
dim(test)

# Build the random forest model using training data
model <- randomForest(price_rate ~ . - price - id - date, data = train, ntree = 1000, mtry = 5)
model
```

There are 17290 and 4323 observations in the training and test dataset, respectively.

Our forest comprises 1000 trees, and we've designated `mtry` as 5, representing the count of randomly chosen variables assessed for potential splits at each stage. The out-of-bag error rate is around 18.72%, indicating the model's estimated error on unseen data.

***Confusion matrix***

The confusion matrix displays the model's predictions for different classes (high, low, medium).

- The 'high' and 'low' classes have higher error rates (23.55% and 28.90%) compared to the 'medium' class (12.64%). This indicates that the model struggles relatively more in accurately predicting these classes.

- For 'high' class predictions, the model has a noticeable error rate in misclassifying instances as 'medium'.

- For 'low' class predictions, the model has a considerable error rate in misclassifying instances as 'medium'.

```{r}
# Validate our model using the test data
prediction <- predict(model, newdata = test)
table(prediction, test$price_rate)
```

The table shows the results of using our model built from the training data to predict the test data.

***Predicted 'high' class:***

Out of the instances predicted as 'high' (801 + 1 + 153 = 955):

- 801 instances were correctly predicted as 'high'.

- 1 instance was mistakenly predicted as 'low'.

- 153 instances were mistakenly predicted as 'medium'.

***Predicted 'low' class:***

Out of the instances predicted as 'low' (0 + 648 + 143 = 791):

- 648 instances were correctly predicted as 'low'.

- No instance was mistakenly predicted as 'high'.

- 143 instances were mistakenly predicted as 'medium'.

***Predicted 'medium' class:***

Out of the instances predicted as 'medium' (211 + 267 + 2099 = 2577):

- 2099 instances were correctly predicted as 'medium'.

- 211 instances were mistakenly predicted as 'high'.

- 267 instances were mistakenly predicted as 'low'.

This table, similar to the confusion matrix, helps to assess the model's performance by illustrating the types of errors it makes when predicting different classes. It gives a detailed breakdown of how the model's predictions align with the actual classes in the test dataset.

```{r}
# Calculate the accuracy of the model
sum(prediction==test$price_rate) / nrow(test)
```


Based on the results from using the model to predict our test data, we can have the accuracy rate of the model as approximately 0.821. This suggests that the model correctly predicts the `price_rate` category for approximately 82.1% of the instances in the test dataset.



Next, we perform a t-test to test whether or not the average prices of houses with waterfront view and those without waterfront view are equal. As shown in the plots above, the variances of these two groups are not identical (where the range of values for houses without waterfront are more spread out than ones with a waterfront view), so we conduct a t-test under the assumption that the variances are unequal.

```{r}
t.test(with_waterfront$price, without_waterfront$price)
```

In our comparison of prices between properties with a waterfront and without a waterfront view, we found our test statistic is $t^* = 12.876$ which is considered to be statistically significant and the difference here is unlikely to have occurred by chance.

Our p-value is less than 2.2e-16, so we have sufficiently strong evidence to reject the null hypothesis at the α = 0.05 significance level. Namely, we have sufficiently strong evidence to conclude that there is a difference between the average price of properties with waterfront view and those do not.

We are 95% confidence that the average price for houses with waterfront view is between 956963.3 less and 1303661.6 more than the average price for houses do not have waterfront view. By 95% confidence level, we mean that if we repeated the study many times, and then constructed many 95% confidence intervals for the difference in average prices between houses with and without waterfront view, then around 95% of those confidence intervals would contain the true difference.

#### Part 3

```{r}
plot(kc_house_data$price ~ kc_house_data$sqft_living)
price <- lm(kc_house_data$price ~ kc_house_data$sqft_living)
abline(price, col = "red")
plot(price$residuals~kc_house_data$sqft_living)
hist(price$residuals)

price <- lm(kc_house_data$price ~ kc_house_data$sqft_living)
summary(price)



plot(kc_house_data$price ~ kc_house_data$sqft_above)
price <- lm(kc_house_data$price ~ kc_house_data$sqft_above)
abline(price, col = "red")
plot(price$residuals~kc_house_data$sqft_above)
hist(price$residuals)

price <- lm(kc_house_data$price ~ kc_house_data$sqft_above)
summary(price)
```

## Technical conditions for SLR
- Linear function: Based on the plot between `price` and `sqft_above`, we can see that our data can roughly fit a line
- Independence of errors: The errors are independent from each other and do not follow any particular trend
- Normally distributed: As we plot the histogram, the errors are normally distributed
- Equal variances: Our errors have equal variances at each value of the predictor.

```{r}
#adam

plot(sqrt(kc_house_data$price) ~ kc_house_data$sqft_above, xlab = "sqft_above", ylab ="price")
price <- lm(sqrt(kc_house_data$price) ~ kc_house_data$sqft_above)
abline(price, col = "red")
plot(price$residuals~kc_house_data$sqft_above)
hist(price$residuals)

price <- lm(kc_house_data$price ~ kc_house_data$sqft_above)
summary(price)
```

## Coefficients Interpretation
According to the result above, the coefficient of `sqft_above` is 268.5, which is pretty large and positive. This means that as the value of `sqft_above` increases, the mean of `price` also tends to increase significantly. If `sqft_above` increases by one square foot, the `price` increases by \$268.5 according to the model. Also, the p-value in this case is less than 2e-16, which indicates the variable `sqft_above` has a significant influence on the `price`. 
From the results, we can have a regression equation: $price = 59953.2 + 268.5*sqft\_above \pm 2.4$


``` {r}
kc_house_data$good_view<-factor(ifelse(kc_house_data$view >= 3,1,0))
kc_house_data_adam <- kc_house_data %>%
  mutate(good_view = as.numeric(good_view == 1))
plot(data = kc_house_data_adam, good_view ~ price)
mod <- glm(data = kc_house_data_adam, good_view ~ kc_house_data$price, family = binomial)
preds <- predict(mod, type = "response")

pred.df <- data.frame(sort(preds), sort(kc_house_data_adam$price))
colnames(pred.df) <- c("probs", "price")
lines(data = pred.df, probs ~ price, col = "red")
summary(mod)
```


``` {r}
kc_house_data$good_view<-factor(ifelse(kc_house_data$view >= 4,1,0))
kc_house_data <- kc_house_data %>%
  mutate(good_view = as.numeric(good_view == 1))
plot(data = kc_house_data, as.numeric(good_view) ~ price)
mod <- glm(data = kc_house_data, as.numeric(good_view) ~ price, family = binomial)
preds <- predict(mod, type = "response")

pred.df <- data.frame(sort(preds), sort(kc_house_data$price))
colnames(pred.df) <- c("probs", "price")
lines(data = pred.df, probs ~ price, col = "red")
summary(mod)
```
### Logistic Regression
In terms of logistic regression, we decided to use `sqft_living` to predict whether the property has a `high_price` or not. Also, we used `mutate` to create a new column to determine if the price of a particular property is above 540088 which is an average price in our data set, then it is considered to have a `high_price`; otherwise, it will be assigned 0. 
``` {r}
kc_house_data$high_price<-factor(ifelse(kc_house_data$price >= 540088,1,0))
kc_house_data <- kc_house_data %>%
  mutate(high_price = as.numeric(high_price == 1))
plot(data = kc_house_data, as.numeric(high_price) ~ sqft_living, ylab = "high_price")
mod <- glm(data = kc_house_data, as.numeric(high_price) ~ sqft_living, family = binomial)
preds <- predict(mod, type = "response")

pred.df <- data.frame(sort(preds), sort(kc_house_data$sqft_living))
colnames(pred.df) <- c("probs", "sqft_living")
lines(data = pred.df, probs ~ sqft_living, col = "red")
summary(mod)
```

Based on the results of the logistic regression model, we can see that the coefficient is 1.859e-03, which is positive. As `sqft_living` increases by one square foot, the log-odds of `high_price` increases by 1.859e-03. 

$\hat{p}(high\_price = 1 | sqft\_living =x) = \frac{e^{-4.513 + 1.859e-03 \times sqft\_living}}{1+e^{-4.513 + 1.859e-03 \times sqft\_living}}$
